{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics in Classification  \n",
    "\n",
    "<img src='images/evaluation.png' width='600' height='450'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for Performance Evaluation  \n",
    "\n",
    "### 1. Introduction  \n",
    "The **Confusion Matrix** is a table that helps evaluate the performance of a classification model. It shows the actual vs. predicted values, helping to analyze errors and performance in a detailed manner.  \n",
    "\n",
    "### 2. Structure of the Confusion Matrix  \n",
    "\n",
    "<img src='images/roc.png' width=250>\n",
    "\n",
    "\n",
    "| Predicted \\ Actual | Positive (1) | Negative (0) |\n",
    "|--------------------|-------------|-------------|\n",
    "| Positive (1)      | **TP**      | **FP**      |\n",
    "| Negative (0)      | **FN**      | **TN**      |\n",
    "\n",
    "- **True Positive (TP):** Model correctly predicts a positive class.  \n",
    "- **False Positive (FP):** Model incorrectly predicts positive instead of negative (Type I Error).  \n",
    "- **False Negative (FN):** Model incorrectly predicts negative instead of positive (Type II Error).  \n",
    "- **True Negative (TN):** Model correctly predicts a negative class.  \n",
    "\n",
    "\n",
    "### 3. Example with Dummy Data  \n",
    "Assume we have **5 samples** where a binary classification model predicts whether a person has a disease (1) or not (0).  \n",
    "\n",
    "#### Actual vs. Predicted Data  \n",
    "| Sample No. | Actual Value | Predicted Value |\n",
    "|------------|-------------|----------------|\n",
    "| 1          | 1           | 1              |\n",
    "| 2          | 0           | 0              |\n",
    "| 3          | 1           | 0              |\n",
    "| 4          | 0           | 1              |\n",
    "| 5          | 1           | 1              |\n",
    "\n",
    "#### Constructing the Confusion Matrix  \n",
    "\n",
    "| Predicted \\ Actual | Positive (1) | Negative (0) |\n",
    "|--------------------|-------------|-------------|\n",
    "| Positive (1)      | 2 (TP)     | 1 (FP)     |\n",
    "| Negative (0)      | 1 (FN)     | 1 (TN)     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Accuracy:\n",
    "\n",
    "### Definition  \n",
    "Accuracy is one of the most basic and commonly used evaluation metrics for classification models. It measures how many predictions a model made correctly out of the total predictions.  \n",
    "\n",
    "### Formula  \n",
    "<img src='images/accuracy.png' width='700'>\n",
    "\n",
    "Where:  \n",
    "- TP (True Positive): Correctly predicted positive instances  \n",
    "- TN (True Negative): Correctly predicted negative instances  \n",
    "- FP (False Positive): Incorrectly predicted positive instances  \n",
    "- FN (False Negative): Incorrectly predicted negative instances  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example  \n",
    "Suppose we have a dataset with 100 samples, and a classification model predicts the labels. The confusion matrix is:  \n",
    "\n",
    "| Predicted \\ Actual | Positive (1) | Negative (0) |\n",
    "|--------------------|-------------|-------------|\n",
    "| Positive (1)      | 45 (TP)     | 5 (FP)      |\n",
    "| Negative (0)      | 10 (FN)     | 40 (TN)     |\n",
    "\n",
    "Now, calculating accuracy:  \n",
    "\n",
    "Accuracy = (45 + 40) / (45 + 40 + 10 + 5)\n",
    "= 85 / 100\n",
    "= 0.85 (85%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Cases Where Accuracy is NOT a Good Metric  \n",
    "\n",
    "#### 1. When Test Data is Imbalanced  \n",
    "- If the dataset is highly imbalanced (one class is much more frequent than the other), accuracy can be misleading.  \n",
    "\n",
    "##### Example:  \n",
    "- Suppose we have a dataset of **1000 samples**, where **950 are class 0 (negative)** and only **50 are class 1 (positive)**.  \n",
    "- A model predicts **all instances as class 0** (negative).  \n",
    "- Accuracy will be:  Accuracy = 950 / 1000 = 0.95 (95%)\n",
    "- Even though the accuracy is **very high (95%)**, the model is completely useless for detecting class 1.  \n",
    "- In such cases, we should use **Precision, Recall, or F1-score** instead.  \n",
    "\n",
    "#### 2. When the Model Returns Probability Scores Instead of Class Labels  \n",
    "- Some models (like Logistic Regression, Neural Networks) output probability scores instead of directly predicting classes.  \n",
    "\n",
    "<img src='images/prob score.png' width=700>\n",
    "\n",
    "##### Example:  \n",
    "- Suppose a model predicts **0.7 probability for class 1** and **0.3 for class 0**, and we set a threshold of **0.5** to classify instances.  \n",
    "- If the threshold is changed (e.g., from 0.5 to 0.8), the classification results will change, affecting accuracy.  \n",
    "- Using accuracy does not consider the probability scores, so **AUC-ROC or Log Loss** is better in such cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision in Classification  \n",
    "\n",
    "### 1. Definition  \n",
    "Precision is a performance metric used in classification models to measure the accuracy of **positive predictions**. It tells us how many of the instances predicted as positive were actually correct.  \n",
    "\n",
    "### 2. Formula  \n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Where:  \n",
    "- **TP (True Positive):** Correctly predicted positive cases.  \n",
    "- **FP (False Positive):** Incorrectly predicted positive cases.  \n",
    "\n",
    "### 3. Example  \n",
    "\n",
    "-  **Scenario:** Spam Email Classification  \n",
    "A model predicts whether an email is spam (1) or not spam (0). Given 100 emails, the confusion matrix is:  \n",
    "\n",
    "| Predicted \\ Actual | Spam (1) | Not Spam (0) |\n",
    "|--------------------|---------|-------------|\n",
    "| Spam (1)          | 40 (TP) | 15 (FP)     |\n",
    "| Not Spam (0)      | 10 (FN) | 35 (TN)     |\n",
    "\n",
    "- **Calculating Precision:**  \n",
    "Precision = TP / (TP + FP) = 40 / (40 + 15) = 40 / 55 = 0.727 (72.7%)\n",
    "\n",
    "\n",
    "4. Interpretation  \n",
    "- **High Precision (close to 1):** The model makes very few false positive errors, meaning it rarely misclassifies non-spam emails as spam.  \n",
    "- **Low Precision (close to 0):** The model has many false positives, meaning it often marks important emails as spam.  \n",
    "\n",
    "\n",
    "6. Limitations of Precision  \n",
    "- Does **not consider false negatives**, which might be important in some cases.  \n",
    "- A model can achieve high precision by predicting fewer positives, but this might **lower recall** (missing actual positive cases).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall in Classification  \n",
    "\n",
    "### 1. Definition  \n",
    "Recall is a performance metric used in classification models to measure how well the model identifies **actual positive instances**. It tells us how many of the **actual positive cases** were correctly predicted.  \n",
    "\n",
    "\n",
    "### 2. Formula  \n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "Where:  \n",
    "- **TP (True Positive):** Correctly predicted positive cases.  \n",
    "- **FN (False Negative):** Incorrectly predicted negative cases.  \n",
    "\n",
    "\n",
    "### 3. Example  \n",
    "\n",
    "**Scenario:** Spam Email Classification  \n",
    "A model predicts whether an email is spam (1) or not spam (0). Given 100 emails, the confusion matrix is:  \n",
    "\n",
    "| Predicted \\ Actual | Spam (1) | Not Spam (0) |\n",
    "|--------------------|---------|-------------|\n",
    "| Spam (1)          | 40 (TP) | 10 (FP)     |\n",
    "| Not Spam (0)      | 15 (FN) | 35 (TN)     |\n",
    "\n",
    "**Calculating Recall:**  \n",
    "Recall = TP / (TP + FN) = 40 / (40 + 15) = 40 / 55 = 0.727 (72.7%)\n",
    "\n",
    "\n",
    "### 4. Interpretation  \n",
    "- **High Recall (close to 1):** The model correctly identifies most actual positive cases (spam emails).  \n",
    "- **Low Recall (close to 0):** The model misses many actual positive cases, meaning many spam emails are left undetected.  \n",
    "\n",
    "\n",
    "### 5. Limitations of Recall  \n",
    "- Does **not consider false positives**, which might be important in some cases.  \n",
    "- A model can achieve high recall by predicting most instances as positive, but this might **lower precision** (increasing false positives).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1-Score in Classification  \n",
    "\n",
    "### 1. Definition  \n",
    "F1-Score is a performance metric used in classification models that balances **Precision** and **Recall**. It is useful when both false positives and false negatives are important.  \n",
    "\n",
    "F1-Score is the **harmonic mean** of Precision and Recall, ensuring that both are considered together.  \n",
    "\n",
    "\n",
    "### 2. Formula  \n",
    "F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "Where:  \n",
    "- **Precision = TP / (TP + FP)**  \n",
    "- **Recall = TP / (TP + FN)**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "\n",
    "The `ROC Curve` (Receiver Operating Characteristic Curve) is a graphical representation that shows the trade-off between True Positive Rate (TPR) and False Positive Rate (FPR) at different classification thresholds.\n",
    "\n",
    "<img src='images/tpr.png' width='400'>\n",
    "\n",
    "### How It Works?\n",
    "- The model predicts probabilities for each class.\n",
    "- By selecting different probability thresholds, we classify instances as positive or negative.\n",
    "- For each threshold, we compute TPR and FPR and plot them on a graph.\n",
    "- The x-axis represents FPR, and the y-axis represents TPR.\n",
    "- The curve shows how well the model separates the positive and negative classes.\n",
    "\n",
    "### Example Calculation:\n",
    "\n",
    "<img src='images/ex.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/read_roc1.png' width='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of ROC Curve\n",
    "- A perfect model: A curve that reaches (0,1) in the top-left corner indicates a perfect classifier (AUC = 1).\n",
    "- A random model: A diagonal line (AUC = 0.5) represents random guessing.\n",
    "- Better models: A curve above the diagonal shows better classification performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC (Area Under the Curve)\n",
    "\n",
    "The AUC (Area Under the ROC Curve) measures the overall performance of a classification model. It represents the probability that a randomly chosen positive instance ranks higher than a randomly chosen negative instance.\n",
    "\n",
    "### AUC Working\n",
    "- Higher AUC (closer to 1): The model performs well in distinguishing between classes.\n",
    "- AUC = 0.5: The model is no better than random guessing.\n",
    "- Lower AUC (close to 0): The model is performing worse than a random classifier (predicting the opposite class).\n",
    "\n",
    "### Interpretation\n",
    "<img src='images/AUC.png' width='700'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
