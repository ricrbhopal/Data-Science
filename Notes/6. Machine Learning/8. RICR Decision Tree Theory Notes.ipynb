{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree in Machine Learning\n",
    "\n",
    "## Definition\n",
    "- A Decision Tree is used for classification and regression tasks.\n",
    "- It is a nested if-else structure for decision-making based on feature values.\n",
    "- The model divides data into subsets using axis-parallel hyperplanes (splits based on one feature).\n",
    "- It predicts categorical outcomes (classification) or continuous values (regression).\n",
    "\n",
    "## Purpose\n",
    "- Breaks down complex decisions into a simpler, interpretable structure.\n",
    "- Provides a clear visual representation of decision-making.\n",
    "- Makes predictions by partitioning the feature space into homogeneous regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tree Structure and Components\n",
    "\n",
    "- Root Node\n",
    "- Internal Nodes\n",
    "- Leaf Nodes\n",
    "- Branches\n",
    "- Splits\n",
    "\n",
    "<img src='images/tree dt.jpeg' width='650px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Intitution:\n",
    "\n",
    "Example: Iris dataset,\n",
    "\n",
    "<img src='images/geo dt.png' width='750px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Decision Tree Creation:\n",
    "\n",
    "<img src='images/tennis ex.png' width='800px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Dichotomiser 3(ID3):\n",
    "\n",
    "1. Calculate Entropy of the entire dataset → 0.94.\n",
    "2. Calculate Entropy of each feature (based on class distributions after split).\n",
    "3. Compute Information Gain (IG) for each feature.\n",
    "4. Feature with highest IG (Outlook) is selected as the root node.\n",
    "5. Repeat steps recursively for remaining branches until leaf nodes are pure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### a. Entropy\n",
    "- Entropy is a measure of the impurity or randomness in the dataset. It helps in deciding the best feature to split at each node.\n",
    "- Formula:  \n",
    "  $\n",
    "  \\text{Entropy}(S) = - \\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
    "  $\n",
    "  Where:\n",
    "  - $ p_i $ is the probability of class $i$ in the set $S$.\n",
    "  - $n$ is the number of distinct classes.\n",
    "\n",
    "<img src='images/entropy.png' width='350px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### b. Information Gain\n",
    "- Information Gain is the reduction in entropy achieved by partitioning the data based on a feature.\n",
    "- It measures how well a feature splits the dataset into pure classes.\n",
    "- Formula:  \n",
    "  $\n",
    "  \\text{Information Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\cdot \\text{Entropy}(S_v)\n",
    "  $\n",
    "  Where:\n",
    "  - $ S $ is the dataset.\n",
    "  - $ A $ is the feature.\n",
    "  - $ S_v $ is the subset of $S$ for which feature $A$ has value $v$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Calculation for \"Play Tennis\" Dataset\n",
    "\n",
    "### Step 1: Entropy of the Entire Dataset\n",
    "The entropy formula:\n",
    "$\n",
    "H(S) = - \\sum p_i \\log_2(p_i)\n",
    "$\n",
    "Where $ p_i $ is the probability of each class.\n",
    "\n",
    "From the dataset:\n",
    "- Yes = 9\n",
    "- No = 5\n",
    "- Total = 14\n",
    "\n",
    "$\n",
    "H(S) = - \\left( \\frac{9}{14} \\log_2 \\frac{9}{14} + \\frac{5}{14} \\log_2 \\frac{5}{14} \\right)\n",
    "$\n",
    "\n",
    "Approximating log values:\n",
    "$\n",
    "H(S) = - (0.642 \\times -0.639 + 0.357 \\times -1.485)\n",
    "$\n",
    "$\n",
    "H(S) = - (-0.41 - 0.53) = 0.94\n",
    "$\n",
    "\n",
    "So, Entropy of the dataset = 0.94.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Entropy of Each Feature\n",
    "#### Feature: Outlook\n",
    "| Outlook  | Yes | No | Total | Entropy |\n",
    "|----------|----|----|------|---------|\n",
    "| Sunny    | 2  | 3  | 5    | $ H = - (2/5 \\log_2 2/5 + 3/5 \\log_2 3/5) $ = 0.971 |\n",
    "| Overcast | 4  | 0  | 4    | $ H = - (4/4 \\log_2 4/4) $ = 0.0 |\n",
    "| Rainy    | 3  | 2  | 5    | $ H = - (3/5 \\log_2 3/5 + 2/5 \\log_2 2/5) $ = 0.971 |\n",
    "\n",
    "$\n",
    "H_{\\text{Outlook}} = \\frac{5}{14} \\times 0.971 + \\frac{4}{14} \\times 0 + \\frac{5}{14} \\times 0.971\n",
    "$\n",
    "$\n",
    "= 0.346 + 0 + 0.346 = 0.693\n",
    "$\n",
    "\n",
    "#### Feature: Humidity\n",
    "| Humidity | Yes | No | Total | Entropy |\n",
    "|----------|----|----|------|---------|\n",
    "| High     | 3  | 4  | 7    | $ H = - (3/7 \\log_2 3/7 + 4/7 \\log_2 4/7) $ = 0.985 |\n",
    "| Normal   | 6  | 1  | 7    | $ H = - (6/7 \\log_2 6/7 + 1/7 \\log_2 1/7) $ = 0.592 |\n",
    "\n",
    "$\n",
    "H_{\\text{Humidity}} = \\frac{7}{14} \\times 0.985 + \\frac{7}{14} \\times 0.592\n",
    "$\n",
    "$\n",
    "= 0.493 + 0.296 = 0.789\n",
    "$\n",
    "\n",
    "#### Feature: Windy\n",
    "| Windy | Yes | No | Total | Entropy |\n",
    "|-------|----|----|------|---------|\n",
    "| False | 6  | 2  | 8    | $ H = - (6/8 \\log_2 6/8 + 2/8 \\log_2 2/8) $ = 0.811 |\n",
    "| True  | 3  | 3  | 6    | $ H = - (3/6 \\log_2 3/6 + 3/6 \\log_2 3/6) $ = 1.0 |\n",
    "\n",
    "$\n",
    "H_{\\text{Windy}} = \\frac{8}{14} \\times 0.811 + \\frac{6}{14} \\times 1.0\n",
    "$\n",
    "$\n",
    "= 0.463 + 0.429 = 0.892\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Information Gain (IG)\n",
    "$\n",
    "IG = H(S) - H(\\text{feature})\n",
    "$\n",
    "\n",
    "| Feature  | Entropy | Information Gain |\n",
    "|----------|---------|-----------------|\n",
    "| Outlook  | 0.693   | $ 0.94 - 0.693 = 0.247 $ |\n",
    "| Humidity | 0.789   | $ 0.94 - 0.789 = 0.151 $ |\n",
    "| Windy    | 0.892   | $ 0.94 - 0.892 = 0.048 $ |\n",
    "\n",
    "- Outlook has the highest Information Gain (0.247), so it is selected as the root node.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Recursive Splitting\n",
    "1. Select Outlook as the root node (highest IG).\n",
    "2. Create branches for each Outlook value:\n",
    "   - Overcast → Always Yes (pure node).\n",
    "   - Sunny & Rainy → Further split based on Humidity & Wind.\n",
    "3. Repeat steps until all leaf nodes are pure.\n",
    "\n",
    "---\n",
    "\n",
    "## LIKE AS:\n",
    "\n",
    "### Step 5: Splitting \"Sunny\" Branch using Humidity\n",
    "We now split the Sunny branch using the Humidity feature.\n",
    "\n",
    "#### Entropy for Humidity in \"Sunny\" branch\n",
    "| Humidity | Yes | No | Total | Entropy |\n",
    "|----------|----|----|------|---------|\n",
    "| High     | 0  | 3  | 3    | $ H = - (3/3 \\log_2 3/3) = 0 $ |\n",
    "| Normal   | 2  | 0  | 2    | $ H = - (2/2 \\log_2 2/2) = 0 $ |\n",
    "\n",
    "$\n",
    "H_{\\text{Sunny}} = \\frac{3}{5} \\times 0 + \\frac{2}{5} \\times 0 = 0\n",
    "$\n",
    "\n",
    "#### Information Gain for Humidity in \"Sunny\"\n",
    "$\n",
    "IG = H(Sunny) - H(Humidity)\n",
    "$\n",
    "\n",
    "$\n",
    "IG = 0.971 - 0 = 0.971\n",
    "$\n",
    "\n",
    "✅ Since IG is 1 (pure split), Humidity is selected, and the Sunny branch is fully classified.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Splitting \"Rainy\" Branch using Windy\n",
    "Next, we split the Rainy branch using the Windy feature.\n",
    "\n",
    "#### Entropy for Windy in \"Rainy\" branch\n",
    "| Windy  | Yes | No | Total | Entropy |\n",
    "|--------|----|----|------|---------|\n",
    "| Weak   | 3  | 0  | 3    | $ H = - (3/3 \\log_2 3/3) = 0 $ |\n",
    "| Strong | 0  | 2  | 2    | $ H = - (2/2 \\log_2 2/2) = 0 $ |\n",
    "\n",
    "$\n",
    "H_{\\text{Rainy}} = \\frac{3}{5} \\times 0 + \\frac{2}{5} \\times 0 = 0\n",
    "$\n",
    "\n",
    "#### Information Gain for Windy in \"Rainy\"\n",
    "$\n",
    "IG = H(Rainy) - H(Windy)\n",
    "$\n",
    "\n",
    "$\n",
    "IG = 0.971 - 0 = 0.971\n",
    "$\n",
    "\n",
    "✅ Since IG is 1 (pure split), Windy is selected, and the Rainy branch is fully classified.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Decision Tree\n",
    "\n",
    "<img src='images/final dt.png' width='550px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gini Impurity\n",
    "- Gini Impurity is another metric to measure the \"impurity\" of a dataset. It helps in splitting data at each node.\n",
    "- It is commonly used in CART (Classification and Regression Trees).\n",
    "- Formula:  \n",
    "  $\n",
    "  \\text{Gini Impurity}(S) = 1 - \\sum_{i=1}^{n} p_i^2\n",
    "  $\n",
    "  Where:\n",
    "  - $ p_i $ is the probability of class $i$ in the set $S$.\n",
    "  - $n$ is the number of distinct classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 100.00%\n",
      "|--- Outlook <= 1.50\n",
      "|   |--- class: 1\n",
      "|--- Outlook >  1.50\n",
      "|   |--- Humidity <= 0.50\n",
      "|   |   |--- class: 0\n",
      "|   |--- Humidity >  0.50\n",
      "|   |   |--- class: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = \"Datasets/play_tennis_dataset.csv\"  # Update with your actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Convert categorical features into numerical values\n",
    "encoder = LabelEncoder()\n",
    "for column in df.columns:\n",
    "    df[column] = encoder.fit_transform(df[column])\n",
    "\n",
    "# Step 3: Split dataset into training and testing sets (80% training, 20% testing)\n",
    "X = df.drop(columns=[\"PlayTennis\"])  # Features\n",
    "y = df[\"PlayTennis\"]  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train Decision Tree Classifier\n",
    "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42,max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate model accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Decision Tree Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 7: Display Decision Tree Rules\n",
    "tree_rules = export_text(model, feature_names=list(X.columns))\n",
    "print(tree_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
